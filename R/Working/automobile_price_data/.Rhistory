names(iris) <- c("SepalLength", "SepalWidth",  "PetalLength", "PetalWidth",  "Species")
subiris <- sqldf("select * from iris where SepalLength > 2.5")
#2.Calculate group wise mean from iris data and perform same using R base counterpart aggregation
sqldf("select Species, AVG(SepalLength) as SepalLength, AVG(SepalWidth) as SepalWidth, AVG(PetalLength) as PetalLength, AVG(PetalWidth) as PetalWidth from subiris group by Species")
aggregate(cbind(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width)~Species, subiris, mean)
#3.Create dept dataframe and emp dataframe and perform inner , leftouter,RightOuter and
#FullOuter Joins using SQLDF
dept <- data.frame(did=c(10,20,30,40),
dname=c("Accounting","Research","Sales","Operations"),
loc=c("New York","Dallas","Boston","Chocago"))
emp <- data.frame(eno=c(7541,5861,5589,6639,7458,2236,7483,8784,9698,7485),
ename=c("Allen","Steve","James","Martin","Alex","Turner","Miller","Scott","Blake","Clark"),
job=c("Analyst","CEO","Clerk","Manager","Salesman","Manager","Analyst","Clerk","Salesman","Salesman"),
mgrid=c(6639,NA,6639,2236,6639,6639,2236,6639,2236,6639),
did=c(30,40,20,10,30,10,20,20,10,30))
sqldf("select * from dept inner join emp on dept.did = emp.did")
sqldf("select * from dept left join emp on dept.did = emp.did")
sqldf("select * from dept right join emp on dept.did = emp.did")
sqldf("select * from dept right outer join emp on dept.did = emp.did")
sqldf("select * from dept rightouter join emp on dept.did = emp.did")
sqldf("select * from dept right join emp on dept.did = emp.did")
sqldf("select * from dept left outer join emp on dept.did = emp.did")               #leftouter
sqldf("select * from dept full outer join emp on dept.did = emp.did")
sqldf("select * from emp right outer join dept on dept.did = emp.did")
sqldf("select * from emp left outer join dept on dept.did = emp.did")
sqldf("select * from dept left outer join emp on dept.did = emp.did union select * from emp left outer join dept on dept.did = emp.did")
sqldf("(select * from dept left outer join emp on dept.did = emp.did) union all (select * from emp left outer join dept on dept.did = emp.did")
sqldf("(select * from dept left outer join emp on dept.did = emp.did) union all (select * from emp left outer join dept on dept.did = emp.did)")
sqldf("select * from dept left outer join emp on dept.did = emp.did union all select * from emp left outer join dept on dept.did = emp.did")
library(dplyr)
subiris %>% group_by('% Species') %>% summarise(count = mean(count))
subiris %>% group_by('% Species')
subiris %>% group_by('% Species') %>% summarise(count = mean(SepalLength))
subiris %>% group_by(Species) %>% summarise(count = mean(SepalLength))
subiris %>% group_by(Species) %>% summarise(count = mean(SepalLength),mean(SepalWidth))
subiris %>% group_by(Species) %>% summarise(count = c(mean(SepalLength),mean(SepalWidth)))
subiris %>% group_by(Species) %>% summarise(count = mean(SepalLength), mean(SepalWidth), mean(PetalLength), mean(PetalWidth))
subiris %>% group_by(Species) %>% summarise(mean(SepalLength), mean(SepalWidth), mean(PetalLength), mean(PetalWidth))
subiris %>% group_by(Species) %>% summarise(SepalLength=mean(SepalLength), SepalWidth=mean(SepalWidth), PetalLength=mean(PetalLength), PetalWidth=mean(PetalWidth))
rm(list = ls())
attach(iris)
#1.install SQLDf package and
#Select the rows from iris dataset where sepal length > 2.5
#and store that in subiris data frame
library(sqldf)
names(iris) <- c("SepalLength", "SepalWidth",  "PetalLength", "PetalWidth",  "Species")
subiris <- sqldf("select * from iris where SepalLength > 2.5")
#2.Calculate group wise mean from iris data and perform same using R base counterpart aggregation
sqldf("select Species, AVG(SepalLength) as SepalLength, AVG(SepalWidth) as SepalWidth, AVG(PetalLength) as PetalLength, AVG(PetalWidth) as PetalWidth from subiris group by Species")
aggregate(cbind(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width)~Species, subiris, mean)
#3.Create dept dataframe and emp dataframe and perform inner , leftouter,RightOuter and
#FullOuter Joins using SQLDF
dept <- data.frame(did=c(10,20,30,40),
dname=c("Accounting","Research","Sales","Operations"),
loc=c("New York","Dallas","Boston","Chocago"))
emp <- data.frame(eno=c(7541,5861,5589,6639,7458,2236,7483,8784,9698,7485),
ename=c("Allen","Steve","James","Martin","Alex","Turner","Miller","Scott","Blake","Clark"),
job=c("Analyst","CEO","Clerk","Manager","Salesman","Manager","Analyst","Clerk","Salesman","Salesman"),
mgrid=c(6639,NA,6639,2236,6639,6639,2236,6639,2236,6639),
did=c(30,40,20,10,30,10,20,20,10,30))
#inner
sqldf("select * from dept inner join emp on dept.did = emp.did")
#leftouter
sqldf("select * from dept left outer join emp on dept.did = emp.did")
#rightouter (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select * from emp left outer join dept on dept.did = emp.did")
#fullouter  (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select * from dept left outer join emp on dept.did = emp.did union all select * from emp left outer join dept on dept.did = emp.did")
#4.Perform above exercise using dplyr package functions
library(dplyr)
subiris %>% group_by(Species) %>% summarise(SepalLength=mean(SepalLength), SepalWidth=mean(SepalWidth), PetalLength=mean(PetalLength), PetalWidth=mean(PetalWidth))
?inner_join
inner_join(dept,emp,by=did)
inner_join(dept,emp)
left_join(dept,emp)
inner_join(dept,emp,"did")
left_join(dept,emp,"did")
inner_join(dept,emp,"did")
left_join(dept,emp,"did")
right_join(dept,emp,"did")
full_join(dept,emp,"did")
dept %>% inner_join(emp)
dept %>% outer_join(emp)
dept %>% full_join(emp)
band_members
band_instruments
rm(list = ls())
attach(iris)
#1.install SQLDf package and
#Select the rows from iris dataset where sepal length > 2.5
#and store that in subiris data frame
library(sqldf)
names(iris) <- c("SepalLength", "SepalWidth",  "PetalLength", "PetalWidth",  "Species")
subiris <- sqldf("select * from iris where SepalLength > 2.5")
#2.Calculate group wise mean from iris data and perform same using R base counterpart aggregation
sqldf("select Species, AVG(SepalLength) as SepalLength, AVG(SepalWidth) as SepalWidth, AVG(PetalLength) as PetalLength, AVG(PetalWidth) as PetalWidth from subiris group by Species")
aggregate(cbind(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width)~Species, subiris, mean)
#3.Create dept dataframe and emp dataframe and perform inner , leftouter,RightOuter and
#FullOuter Joins using SQLDF
dept <- data.frame(did=c(10,20,30,40,50),
dname=c("Accounting","Research","Sales","Operations","HR"),
loc=c("New York","Dallas","Boston","Chocago","Boston"))
emp <- data.frame(eno=c(7541,5861,5589,6639,7458,2236,7483,8784,9698,7485),
ename=c("Allen","Steve","James","Martin","Alex","Turner","Miller","Scott","Blake","Clark"),
job=c("Analyst","CEO","Clerk","Manager","Salesman","Manager","Analyst","Clerk","Salesman","Salesman"),
mgrid=c(6639,NA,6639,2236,6639,6639,2236,6639,2236,6639),
did=c(30,40,20,10,30,10,20,20,10,30))
#inner
sqldf("select * from dept inner join emp on dept.did = emp.did")
#leftouter
sqldf("select * from dept left outer join emp on dept.did = emp.did")
#rightouter (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select * from emp left outer join dept on dept.did = emp.did")
#fullouter  (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select * from dept left outer join emp on dept.did = emp.did union all select * from emp left outer join dept on dept.did = emp.did")
#4.Perform above exercise using dplyr package functions
library(dplyr)
subiris %>% group_by(Species) %>% summarise(SepalLength=mean(SepalLength), SepalWidth=mean(SepalWidth), PetalLength=mean(PetalLength), PetalWidth=mean(PetalWidth))
inner_join(dept,emp,"did")
left_join(dept,emp,"did")
right_join(dept,emp,"did")
full_join(dept,emp,"did")
sqldf("select * from dept left outer join emp on dept.did = emp.did union select * from emp left outer join dept on dept.did = emp.did")
rm(list = ls())
attach(iris)
#1.install SQLDf package and
#Select the rows from iris dataset where sepal length > 2.5
#and store that in subiris data frame
library(sqldf)
names(iris) <- c("SepalLength", "SepalWidth",  "PetalLength", "PetalWidth",  "Species")
subiris <- sqldf("select * from iris where SepalLength > 2.5")
#2.Calculate group wise mean from iris data and perform same using R base counterpart aggregation
sqldf("select Species, AVG(SepalLength) as SepalLength, AVG(SepalWidth) as SepalWidth, AVG(PetalLength) as PetalLength, AVG(PetalWidth) as PetalWidth from subiris group by Species")
aggregate(cbind(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width)~Species, subiris, mean)
#3.Create dept dataframe and emp dataframe and perform inner , leftouter,RightOuter and
#FullOuter Joins using SQLDF
dept <- data.frame(did=c(10,20,30,40,50),
dname=c("Accounting","Research","Sales","Operations","HR"),
loc=c("New York","Dallas","Boston","Chocago","Boston"))
emp <- data.frame(eno=c(7541,5861,5589,6639,7458,2236,7483,8784,9698,7485,4466),
ename=c("Allen","Steve","James","Martin","Alex","Turner","Miller","Scott","Blake","Clark","Neil"),
job=c("Analyst","CEO","Clerk","Manager","Salesman","Manager","Analyst","Clerk","Salesman","Salesman","Security"),
mgrid=c(6639,NA,6639,2236,6639,6639,2236,6639,2236,6639,2236),
did=c(30,40,20,10,30,10,20,20,10,30,60))
#inner
sqldf("select * from dept inner join emp on dept.did = emp.did")
#leftouter
sqldf("select * from dept left outer join emp on dept.did = emp.did")
#rightouter (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select * from emp left outer join dept on dept.did = emp.did")
#fullouter  (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select * from dept left outer join emp on dept.did = emp.did union select * from emp left outer join dept on dept.did = emp.did")
#4.Perform above exercise using dplyr package functions
library(dplyr)
subiris %>% group_by(Species) %>% summarise(SepalLength=mean(SepalLength), SepalWidth=mean(SepalWidth), PetalLength=mean(PetalLength), PetalWidth=mean(PetalWidth))
inner_join(dept,emp,"did")
left_join(dept,emp,"did")
right_join(dept,emp,"did")
full_join(dept,emp,"did")
#inner
sqldf("select dept.*, emp.* from dept inner join emp on dept.did = emp.did")
#leftouter
sqldf("select dept.*, emp.* from dept left outer join emp on dept.did = emp.did")
#rightouter (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select dept.*, emp.* from emp left outer join dept on dept.did = emp.did")
#fullouter  (edited to overcome "RIGHT and FULL OUTER JOINs are not currently supported" error)
sqldf("select dept.*, emp.* from dept left outer join emp on dept.did = emp.did union select * from emp left outer join dept on dept.did = emp.did")
?merge
?match
?cbind
cbind(dept[match(names(value), emp$did),], value)
cbind(dept[match(dept$did, emp$did),], value)
cbind(dept[match(dept$did, emp$did),], did)
cbind(dept[match(dept$did, emp$did),], dept$did)
cbind(dept[match(dept$did, emp$did),], )
cbind(dept[match(dept$did, emp$did),])
match(dept$did,emp$did)
dept %>% inner_join(emp)
dept %>% inner_join(emp)
dept %>% left_join(emp)
dept %>% right_join(emp)
dept %>% full_join(emp)
merge(dept, emp, by = "did")
merge(dept, emp, by = "did", all = TRUE)
merge(dept, emp, by = "did", all.x = TRUE)
merge(dept, emp, by = "did", all.y = TRUE)
rm(list = ls())
rm(ls())
b=0
rm(ls())
rm(list = ls())
---
title: "knn"
author: "Adwesh Behera - 20025006"
date: "11 September 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r cars}
summary(cars)
```
## Including Plots
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
text = "all of my film reviews are archived at http : //us . imdb . com/m/reviews_by ? justin + felix this review has been submitted to the shrubbery. inspired by the short story the legend of sleepy hollow by washington irving .
quaid stars as a man who has taken up the proffesion of dragonslayer after he feels he is betrayed by a dragon early in the movie . he runs into the last dragon in existence , and there is a genuinely amusing battle between the two which results in a standoff where quaid is in the dragons mouth , but has his sword pointed at the dragons brain . eventually , they decide to call a truce , and they work out a deal . since he is the last dragon , he will pretend to die and quaid will be able to get paid for it .
starring : val kilmer ( bruce wayne/batman ) , jim carrey ( edward nygma/the riddler ) , tommy lee jones ( harvey dent/two-face ) , chris o'donnell ( dick grayson/robin ) , nicole kidman ( dr . chase meridian ) directed by : joel schumacher , written by : lee batchler & janet scott batchler and akiva goldsman , based on characters created by bob kane rated pg-13 by the mpaa for violence , strong language , and a sexual implication or two
directed by : roland emmerich , written by : dean devlin & roland emmerichrated pg-13 for violence , mild profanity
written by sidney howard ; produced by david o . selznik ; directed by victor fleming ; based on the novel by margaret mitchell . seen july 8 , 1998 at the crossgates cinema 18 , ( albany , ny ) , theater #7 , at 8 : 15 p . m . with my mom using hoyts cinema cash . [theater rating : * * * 1/2 : very good sound , picture , and seats]
vincent grows up weaker , shorter and less graceful than his younger brother anton . if that weren't enough , mom and dad treat the boys differently , as if vincent might break from the same fall that anton brushes off . understandably , vincent grows up wanting to get as far away from home - and from people - as possible , all the way to the moons of jupiter .
the disney studios has its formula for annual , full-length animated features down so pat that it's hard to remember which one you're watching at any given moment . once again we have a tale focused around a strong central character . add the requisite love interest and a wise-cracking sidekick or two , pep up the soundtrack with a handful of strategically placed show tunes ( an introspective number by a pool or looking in a mirror , a rousing anthem , a cutesy , montage-backed crowd pleaser ) , pose a few threats to our engaging lead , and tie things all up neatly by the closing credits .
it's been done throughout film history over and over again , and has become one of the many sub-genres , like the prep school sub-genre , the angst-filled teen sub-genre , and the slasher sub-genre . what makes " peter's friends " so remarkable is not how it doesn't follow the rules of this sub-genre , but how it follows it so well , and creates characters that i wouldn't mind hanging with . what are the rules of the reunion sub-genre ? well . . . . 1 . you need a group of friends , most of them married or at least dating.
the first thing you notice about this movie is that it's cold . placed in minnesota and north dakota during the winter , many of the scenes take place outside with long scenes of snow-covered ground against a background of white . just what we need as bloomington struggles out of the endless long cold night. in brief scenes of vicious violence , people get shot through the head , in the back , face , chest and various other areas of the anatomy . there is also one of the most unpleasant body disposal scenes yet seen on the screen . remarkably enough , most of the movie is played for laughs .
i had been expecting more of this movie than the less than thrilling twister . twister was good but had no real plot and no one to simpithize with .. . . i liked this movie . . . but it was not as great as i hoped . i was still good none the less . it had excellent special effects . the best view . . . the helecopters flying over the streets of volcanos . also . . . there were interesting side stories that made the plot more interesting . so . . . it was good.
he is duncan macleod of the clan macleod . he's been pimpin' it since he was born in the village of glennfillan in 15somethingsomething , and he continues to pimp it in modern day . he is immortal and he cannot die .
a michael jordan fan's heartbreak
these are words that could be used to describe the emotions of john sayles' characters in his latest , limbo . but no , i use them to describe myself after sitting through his latest little exercise in indie egomania . i can forgive many things . but using some hackneyed , whacked-out , screwed-up * non * -ending on a movie is unforgivable . i walked a half-mile in the rain and sat through two hours of typical , plodding sayles melodrama to get cheated by a complete and total copout finale . does sayles think he's roger corman ?
rarely have to so divergent reviews for one movie crossed my desk on the same day . to wit , we present a unique experience
running time : 92 minutes note : some may consider portions of the following text to be spoilers . be forewarned . -------------------------------------------------------------to assess alex cox's film the winner as a loser would be so indolent . it would be derisive . it would be glib . it would be dismissive
the following review contains some harsh language . . . but what did you expect when you clicked on this title ?  the thought of losing you makes me all vomity inside .
annie wilson ( cate blanchett ) , a widow who struggles to raise her children in a small town in georgia is asked for help by local authorities in solving the case of a missing woman . annie is something of a psychic , she has involuntary bouts with the supernatural where she can see the past and the future and physically feel the actions happenning to her she envisions . her " gift " leads to the arrest of a nasty wife beater who may or may not have killed the pretty rich girl found in the swamp on his property . in an ultra conservative backwoods town will her testimony based on her psychic visions hold up in court ? do they even have the right man ?
in duets , karaoke bars dominate the american western landscape like taco bells and starbucks . they're in every major city and full of hot , young people swaying while marginally talented participants sing weather girls covers . karaoke is a craze , the way dandruff or waxy ears are a craze . i like to think i'm pretty pop culture savvy , thanks to years of reading rolling stone and entertainment weekly . but i don't remember reading one article about karaoke clubs being the discos of our times .
everything in the phantom you have seen many times before and there is nothing new presented here . wincer displays absolutely no skill in setting up an exciting action sequence . billy zane is wooden as the hero . kristy swanson is given very little to do , and does very little with it . treat williams , looking like rhett butler but sounding like mickey mouse , is one of the worst villains i have ever seen in a movie . only catherine zeta jones , as one of williams cohorts turns in a good performance . however , if youre looking for a fun family movie , go watch the underrated flipper . this is not a good movie .
"expectations " may be an obsession of mine , but i'll say it : * if you can park virtually every expectation at the door * , this will be a flick that can become addictive . it will hit you on more levels than most three other movies . it's a romp--the sort of thing lots of us wanted to do as kids--screw everything , grab a car , and beat it for the border . it's an adventure--where the stakes are high , and the hero must play everything right or face disaster . it's a comedy--where adults are somewhat silly , and two kids create their own universe . it's sort of like a horror story , but without magic or the supernatural--where the demons are utterly ordinary people doing ordinary sorts of things . and it's a morality play--where the hero has to face himself and his own actions . but if you can't park the expectations , you may very well hit a wall with this film . others have .
the following review encompasses two versions of dune : dune : the theatrical version ( 1984 ) runtime : 137 minutes capsule review : cut down to just over two hours by nervous studio executives , the theatrical version of dune is a spectacular mess and may be incomprehensible to those unfamiliar with the book . the film's visual splendour , mystical beauty and impressive action scenes only partly compensate for gaping holes in the narrative
a hollywood production office--plush , swank , business-like . interior . day . writer jim kouf is pitching his idea . he draws an imaginary theater marquee in the air and announces proudly , " stakeout ii . it's perfect . think
the premise of turbulence is i'm sure very familiar to us all . we've seen it before in passenger 57 , executive decision , and countless other flicks that are good in there own way . you know , terrorists take over a plane , ask the police on the ground for what they what and so on . turbulence starts out with a " convict " ray liotta being accused of a crime .
make a likable trio of protagonists , but they're just about the only palatable element of the mod squad , a lame-brained big-screen version of the 70s tv show . the story has all the originality of a block of wood ( well , it would if you could decipher it ) , the characters are all blank slates , and scott silver's perfunctory action sequences are as cliched as they come . by sheer force of talent , the three actors wring marginal enjoyment from the proceedings whenever they're on screen , but the mod squad is just a second-rate action picture with a first-rate cast . "
# 1. Doughnut Plot
data = c(179718,41370,41914,44280)
# 1. Doughnut Plot
library(ggplot2)
data = c(179718,41370,41914,44280)
ggplot(data)
ggplot(as.data.frame(data))
ggplot(as.data.frame(data)) + geom_point()
data = c("Army", "Navy", "Airforce", "Marine", 179718, 41370, 41914, 44280)
data = matrix(c("Army", "Navy", "Airforce", "Marine", 179718, 41370, 41914, 44280), ncol = 2)
data = c(179718,41370,41914,44280)da
data
data[,2] = as.numeric(data[,2])
data
View(data)
data[,2] = as.numeric(data[,2])
dara
data
data[1.2]
data[1,2]
as.numeric(data[1,2])
as.numeric(daa[,2])
as.numeric(data[,2])
data[,2] = as.numeric(data[,2])
data
as.data.frame(data)
data = as.data.frame(matrix(c("Army", "Navy", "Airforce", "Marine", 179718, 41370, 41914, 44280), ncol = 2))
data
ggplot(data, aes(x=2, y=V2, fill=type))
source('~/.active-rstudio-document', echo=TRUE)
doughnut <-
function (x, labels = names(x), edges = 200, outer.radius = 0.8,
inner.radius=0.6, clockwise = FALSE,
init.angle = if (clockwise) 90 else 0, density = NULL,
angle = 45, col = NULL, border = FALSE, lty = NULL,
main = NULL, ...)
{
if (!is.numeric(x) || any(is.na(x) | x < 0))
stop("'x' values must be positive.")
if (is.null(labels))
labels <- as.character(seq_along(x))
else labels <- as.graphicsAnnot(labels)
x <- c(0, cumsum(x)/sum(x))
dx <- diff(x)
nx <- length(dx)
plot.new()
pin <- par("pin")
xlim <- ylim <- c(-1, 1)
if (pin[1L] > pin[2L])
xlim <- (pin[1L]/pin[2L]) * xlim
else ylim <- (pin[2L]/pin[1L]) * ylim
plot.window(xlim, ylim, "", asp = 1)
if (is.null(col))
col <- if (is.null(density))
palette()
else par("fg")
col <- rep(col, length.out = nx)
border <- rep(border, length.out = nx)
lty <- rep(lty, length.out = nx)
angle <- rep(angle, length.out = nx)
density <- rep(density, length.out = nx)
twopi <- if (clockwise)
-2 * pi
else 2 * pi
t2xy <- function(t, radius) {
t2p <- twopi * t + init.angle * pi/180
list(x = radius * cos(t2p),
y = radius * sin(t2p))
}
for (i in 1L:nx) {
n <- max(2, floor(edges * dx[i]))
P <- t2xy(seq.int(x[i], x[i + 1], length.out = n),
outer.radius)
polygon(c(P$x, 0), c(P$y, 0), density = density[i],
angle = angle[i], border = border[i],
col = col[i], lty = lty[i])
Pout <- t2xy(mean(x[i + 0:1]), outer.radius)
lab <- as.character(labels[i])
if (!is.na(lab) && nzchar(lab)) {
lines(c(1, 1.05) * Pout$x, c(1, 1.05) * Pout$y)
text(1.1 * Pout$x, 1.1 * Pout$y, labels[i],
xpd = TRUE, adj = ifelse(Pout$x < 0, 1, 0),
...)
}
## Add white disc
Pin <- t2xy(seq.int(0, 1, length.out = n*nx),
inner.radius)
polygon(Pin$x, Pin$y, density = density[i],
angle = angle[i], border = border[i],
col = "white", lty = lty[i])
}
title(main = main, ...)
invisible(NULL)
}
doughnut(data[,2], data[,1])
data = c(179718,41370,41914,44280)
double(data, c("Army","Navy","Airforce","Marine"))
doughnut(data)
doughnut(data, labels = c("a","b","c","d"))
doughnut(data, labels = c("Army","Navy","Airforce","Marine"))
doughnut(data, labels = c("Army","Navy","Airforce","Marine"), main = "Traumatic Brain Injury")
# 1. Doughnut Plot
library(ggplot2)
data = c(179718,41370,41914,44280)
data = as.data.frame(matrix(c("Army", "Navy", "Airforce", "Marine", 179718, 41370, 41914, 44280), ncol = 2))
data
rm(list = ls())
setwd("Desktop/DesktopDocuments/TopGearTemp/R/Working/automobile_price_data/")
data = read.csv("Automobile price data _Raw.csv")
# Replace '?' in the data with NA
data[data == "?"] = NA
na_count_col = as.data.frame(sapply(data, function(y) sum(length(which(is.na(y))))))
na_count_col
# I decieded to remove normalized.losses column as the comumn has 41 missing values
data = data[,-2]
na_count_row = rowSums(is.na(data))
na_count_row
sum(na_count_row > 0)
# I decieded to remove 12 rows with NA value from the dataset
data = data[complete.cases(data),]
# I need to convert categorical variables to numerical data
factor_columns = which(as.numeric(sapply(data, is.factor)) == 1)
for (i in factor_columns) {
data[,i] = as.numeric(data[,i])
}
str(data)
# Normalization
scaled.data = as.data.frame(scale(data[,-25]))
scaled.data$price = data$price
# Breaking down to 80% training and 20% testing dataset
samples = sample(1:193, 39)
training = scaled.data[-samples,]
testing = scaled.data[samples,]
# Model Training with LInear regression
# We take all the features into account
features = names(scaled.data)
features = paste(features[!features %in% "price"], collapse = "+")
formula = paste("price ~ ", features, collapse = "+")
formula = as.formula(formula)
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.05, stepmax = 1e+9, data = training)
plot(NN)
# Prediction
predictions = compute(NN, testing[,1:24])
# Error Calculations
# 1.Mean Absolute Error
sum(abs(predictions$net.result - testing$price)) / nrow(testing)
# 2. RMS Error
(sum((predictions$net.result - testing$price)**2) / nrow(testing)) ** 0.5
# 3. Relative Absolute Error
T.bar = sum(testing$price)/nrow(testing)
sum(abs(predictions$net.result - testing$price)) / sum(abs(testing$price - T.bar))
# 4. Relative Squared Error
(sum((predictions$net.result - testing$price)**2) / sum((testing$price - T.bar)**2)) ** 0.5
# Coefficient of Determination
pred.bar = mean(predictions$net.result)
pred.sd = sd(predictions$net.result)
test.bar = mean(testing$price)
test.sd = sd(testing$price)
(sum((predictions$net.result-pred.bar)*(testing$price-test.bar)) / (nrow(testing)*pred.sd*test.sd)) ** 2
rm(list = ls())
setwd("Desktop/DesktopDocuments/TopGearTemp/R/Working/automobile_price_data/")
data = read.csv("Automobile price data _Raw.csv")
# Replace '?' in the data with NA
data[data == "?"] = NA
na_count_col = as.data.frame(sapply(data, function(y) sum(length(which(is.na(y))))))
na_count_col
# I decieded to remove normalized.losses column as the comumn has 41 missing values
data = data[,-2]
na_count_row = rowSums(is.na(data))
na_count_row
sum(na_count_row > 0)
# I decieded to remove 12 rows with NA value from the dataset
data = data[complete.cases(data),]
# I need to convert categorical variables to numerical data
factor_columns = which(as.numeric(sapply(data, is.factor)) == 1)
for (i in factor_columns) {
data[,i] = as.numeric(data[,i])
}
str(data)
# Normalization
scaled.data = as.data.frame(scale(data[,-25]))
scaled.data$price = data$price
# Breaking down to 80% training and 20% testing dataset
samples = sample(1:193, 39)
training = scaled.data[-samples,]
testing = scaled.data[samples,]
# Model Training with LInear regression
# We take all the features into account
features = names(scaled.data)
features = paste(features[!features %in% "price"], collapse = "+")
formula = paste("price ~ ", features, collapse = "+")
formula = as.formula(formula)
library(neuralnet)
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.05, stepmax = 1e+9, data = training)
plot(NN)
# Prediction
predictions = compute(NN, testing[,1:24])
# Error Calculations
# 1.Mean Absolute Error
sum(abs(predictions$net.result - testing$price)) / nrow(testing)
# 2. RMS Error
(sum((predictions$net.result - testing$price)**2) / nrow(testing)) ** 0.5
# 3. Relative Absolute Error
T.bar = sum(testing$price)/nrow(testing)
sum(abs(predictions$net.result - testing$price)) / sum(abs(testing$price - T.bar))
# 4. Relative Squared Error
(sum((predictions$net.result - testing$price)**2) / sum((testing$price - T.bar)**2)) ** 0.5
# Coefficient of Determination
pred.bar = mean(predictions$net.result)
pred.sd = sd(predictions$net.result)
test.bar = mean(testing$price)
test.sd = sd(testing$price)
(sum((predictions$net.result-pred.bar)*(testing$price-test.bar)) / (nrow(testing)*pred.sd*test.sd)) ** 2
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.05, stepmax = 1e+9, data = training)
rm(list = ls())
rm(list = ls())
setwd("Desktop/DesktopDocuments/TopGearTemp/R/Working/automobile_price_data/")
data = read.csv("Automobile price data _Raw.csv")
# Replace '?' in the data with NA
data[data == "?"] = NA
na_count_col = as.data.frame(sapply(data, function(y) sum(length(which(is.na(y))))))
na_count_col
# I decieded to remove normalized.losses column as the comumn has 41 missing values
data = data[,-2]
na_count_row = rowSums(is.na(data))
na_count_row
sum(na_count_row > 0)
# I decieded to remove 12 rows with NA value from the dataset
data = data[complete.cases(data),]
# I need to convert categorical variables to numerical data
factor_columns = which(as.numeric(sapply(data, is.factor)) == 1)
for (i in factor_columns) {
data[,i] = as.numeric(data[,i])
}
str(data)
# Normalization
scaled.data = as.data.frame(scale(data[,-25]))
scaled.data$price = data$price
# Breaking down to 80% training and 20% testing dataset
samples = sample(1:193, 39)
training = scaled.data[-samples,]
testing = scaled.data[samples,]
# Model Training with LInear regression
# We take all the features into account
features = names(scaled.data)
features = paste(features[!features %in% "price"], collapse = "+")
formula = paste("price ~ ", features, collapse = "+")
formula = as.formula(formula)
library(neuralnet)
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.05, stepmax = 1e+9, data = training)
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.05, stepmax = 1e+9, data = training)
plot(NN)
predictions = compute(NN, testing[,1:24])
predictions$net.result
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.01, stepmax = 1e+9, data = training)
NN = neuralnet(formula = formula, hidden = c(20,10,5), linear.output = T, threshold = 0.1, stepmax = 1e+9, data = training)
plot(NN)
# Prediction
predictions = compute(NN, testing[,1:24])
predictions$net.result
# Error Calculations
# 1.Mean Absolute Error
sum(abs(predictions$net.result - testing$price)) / nrow(testing)
# 2. RMS Error
(sum((predictions$net.result - testing$price)**2) / nrow(testing)) ** 0.5
# 3. Relative Absolute Error
T.bar = sum(testing$price)/nrow(testing)
sum(abs(predictions$net.result - testing$price)) / sum(abs(testing$price - T.bar))
# 4. Relative Squared Error
(sum((predictions$net.result - testing$price)**2) / sum((testing$price - T.bar)**2)) ** 0.5
# Coefficient of Determination
pred.bar = mean(predictions$net.result)
pred.sd = sd(predictions$net.result)
test.bar = mean(testing$price)
test.sd = sd(testing$price)
(sum((predictions$net.result-pred.bar)*(testing$price-test.bar)) / (nrow(testing)*pred.sd*test.sd)) ** 2
